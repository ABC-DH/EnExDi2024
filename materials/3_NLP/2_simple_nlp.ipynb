{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z74vm47glyC"
      },
      "source": [
        "#**NLP : tokenizing, lemmatizing, postagging**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt0eU-qsLb10"
      },
      "source": [
        "## What's NLP and why do we need it ?\n",
        "Generally speaking, loads of people use NLP as a preprocessing phase, for further textual treatment. And it is absolutely necessary if you want to avoid noise in statistical analysis and machine learning uses. Basically, preprocessing is **tokenizing**, **lemmatizing** and **postagging**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLIKh_80WtdM"
      },
      "source": [
        "## How do you do that without programming ?\n",
        "I'll show you some basic tools you can easily use without knowing programming in python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31i7LBEP1HF9"
      },
      "source": [
        "##**Some useful online tools**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P_eXTzr1Uup"
      },
      "source": [
        "###**UDPipe**\n",
        "You'll find it [here](https://lindat.mff.cuni.cz/services/udpipe/).\n",
        "<br>You can use that for short texts.\n",
        "<br>One of its peculiarities is SVG dependency tree building."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDSIHDth4kxu"
      },
      "source": [
        "###**Deucalion**\n",
        "You'll find it [here](https://dh.chartes.psl.eu/deucalion/api/fr/).\n",
        "<br>Much more accurate for longer texts, not an easy output (although ready-to-use format). It's really good on Ancient French and Latin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbFh1tm45B9B"
      },
      "source": [
        "###**VoyantTools**\n",
        "You'll find it [here](https://voyant-tools.org/).\n",
        "<br>It's a visualization, directly online, but for more modules you can build it locally and it's really powerful and neat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj7F-HDpcCkK"
      },
      "source": [
        "#**LE TAL : TOKENISATION, LEMMATISATION, POSTAGGING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQIzGqJzcUVk"
      },
      "source": [
        "We're going to test **`stanza`**. There are loads of other modules on the matter (like`spacy` and `pie-extended`), but `stanza` generally outperforms them for accuracy and efficiency."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "MHhsLVo_LxjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VR5RRUfQc5u4"
      },
      "outputs": [],
      "source": [
        "catilinaires=\"Quousque tandem abutere, Catilina, patientia nostra ? Quamdiu etiam furor iste tuus nos eludet ? Quem ad finem sese effrenata jactabit audacia ? Nihilne te nocturnum praesidium Palatii, nihil urbis vigiliae, nihil timor populi, nihil concursus bonorum omnium, nihil hic munitissimus habendi senatus locus, nihil horum ora vultusque moverunt ? Patere tua consilia non sentis ? Constrictam jam horum omnium scientia teneri conjurationem tuam non vides ? Quid proxima, quid superiore nocte egeris, ubi fueris, quos convocaveris, quid consilii ceperis, quem nostrum ignorare arbitraris ? O tempora ! O mores ! Senatus haec intellegit, consul videt. Hic tamen vivit.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r_DV1BTw4Cr"
      },
      "source": [
        "##**stanza (précédemment Stanford CoreNLP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPEkXPIxeUob"
      },
      "source": [
        "`stanza` has several language models at your disposal (here's a [list](https://stanfordnlp.github.io/stanza/performance.html)), which you can get using the basic language code, like `grc` for Ancient Greek or `la` for Latin. But you can also specify which model you want like below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJXi9pljw2DR"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "stanza.download('la', package=\"perseus\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKCT-NaPxpbp"
      },
      "source": [
        "We begin with building a Pipeline, to indicate which processors we want to use (no need to add `ner` if you don't need named entity recognition)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x5kKE7rxTgB"
      },
      "outputs": [],
      "source": [
        "nlp_stanza = stanza.Pipeline(lang='la', package=\"perseus\", processors='tokenize,pos,lemma, depparse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUd62DkjgcvW"
      },
      "source": [
        "Now you can launch the nlp process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TMPY243ZyE8S"
      },
      "outputs": [],
      "source": [
        "catilinaires_analyzed=nlp_stanza(catilinaires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSS-MkX_gr7m"
      },
      "source": [
        "Here are some results, first for sentence division, and then for lemmatizing and postagging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQPdO0otBqA8"
      },
      "outputs": [],
      "source": [
        "for sent in catilinaires_analyzed.sentences:\n",
        "  print(\"XXXXX \"+sent.text+\" XXXXX\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNXzUVFZzojD"
      },
      "outputs": [],
      "source": [
        "for sent in catilinaires_analyzed.sentences:\n",
        "  for token in sent.words:\n",
        "    print(token.text + ' - ' + token.lemma + ' - ' + token.pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try with a (much bigger) text."
      ],
      "metadata": {
        "id": "etySMAjIWQZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the text we're going to use is very long, we will use batch processing, that is, launching several processes at once for GPU management."
      ],
      "metadata": {
        "id": "au9q53YfJiCD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vHGyvw9k7HD3"
      },
      "outputs": [],
      "source": [
        "def batch_process(text, nlp, batch_size=50):\n",
        "    paragraphs = text.split('\\n')\n",
        "    batches = [paragraphs[i:i + batch_size] for i in range(0, len(paragraphs), batch_size)]\n",
        "\n",
        "    words = []\n",
        "\n",
        "    for batch in batches:\n",
        "        batch_text = '\\n'.join(batch)\n",
        "        doc = nlp(batch_text)\n",
        "        for sentence in doc.sentences:\n",
        "            for word in sentence.words:\n",
        "                token={}\n",
        "                if word.lemma is not None:\n",
        "                    token[\"word\"]=word.text\n",
        "                    token[\"lemma\"]=word.lemma\n",
        "                    token[\"pos\"]=word.pos\n",
        "                    words.append(token)\n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('fr')\n",
        "import string"
      ],
      "metadata": {
        "id": "f2bHYAFOYfpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get the _Misérables_."
      ],
      "metadata": {
        "id": "D-eWZAZJKWaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ABC-DH/EnExDi2024/main/materials/3_NLP/miserables.txt"
      ],
      "metadata": {
        "id": "zgUE5AS2NVSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_of_text = \"/content/miserables.txt\""
      ],
      "metadata": {
        "id": "zCCJtvF2ZP1y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = open(filepath_of_text, encoding=\"utf-8\").read()"
      ],
      "metadata": {
        "id": "Jb1ll9BDZS6s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_stanza = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos,lemma')"
      ],
      "metadata": {
        "id": "bAPzFT46YugW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part may take some time (for me it took something like 4 minutes)."
      ],
      "metadata": {
        "id": "ZGmbsLUeMxac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "miserables_analyzed = batch_process(full_text, nlp_stanza)"
      ],
      "metadata": {
        "id": "xIBpz_4nYjkQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(miserables_analyzed[5:15])"
      ],
      "metadata": {
        "id": "x0tlOmanZaAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For your own projects, you can get loads of stopword lists [here](https://github.com/stopwords-iso).\n"
      ],
      "metadata": {
        "id": "QRR7NZ8naKEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ABC-DH/EnExDi2024/main/materials/3_NLP/stopwords_fr.txt"
      ],
      "metadata": {
        "id": "93Do-k0Ja05o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = open(\"/content/stopwords_fr.txt\",'r',encoding=\"utf8\").read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "-M_2igj5a2KQ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forms = []\n",
        "lemmas = []\n",
        "no_stop = []\n",
        "\n",
        "for token in miserables_analyzed:\n",
        "    form = token[\"word\"]\n",
        "    lemma = token[\"lemma\"]\n",
        "\n",
        "    if lemma not in string.punctuation:\n",
        "        forms.append(form)\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "    if lemma not in string.punctuation and lemma not in stopwords:\n",
        "        no_stop.append(lemma)"
      ],
      "metadata": {
        "id": "KQMQx7NTa_Ah"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(lemmas)"
      ],
      "metadata": {
        "id": "nRAm6ZDsdHBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(no_stop)"
      ],
      "metadata": {
        "id": "qGTu3w4hdI4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we're going to build some basic representations of our text to see why preprocessing is important."
      ],
      "metadata": {
        "id": "7yHqu69sLbq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def create_word_cloud(words_list, title):\n",
        "    text = ' '.join(words_list)\n",
        "\n",
        "    radius = 495\n",
        "    diameter = radius * 2\n",
        "    center = radius\n",
        "    x, y = np.ogrid[:diameter, :diameter]\n",
        "    mask = (x - center) ** 2 + (y - center) ** 2 > radius ** 2\n",
        "    mask = 255 * mask.astype(int)\n",
        "\n",
        "    mask_rgba = np.dstack((mask, mask, mask, 255 - mask))\n",
        "\n",
        "    wordcloud = WordCloud(repeat=False, width=diameter, height=diameter,\n",
        "                          background_color=None, mode=\"RGBA\", colormap='plasma',\n",
        "                          mask=mask_rgba).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9qv1DH7LdMhA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(forms, 'Word Cloud for Forms')"
      ],
      "metadata": {
        "id": "h-wt9rK1eJpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(lemmas, 'Word Cloud for Lemmas')"
      ],
      "metadata": {
        "id": "t5B5uCAEeL5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(no_stop, 'Word Cloud for Lemmas without stopwords')"
      ],
      "metadata": {
        "id": "pKzLFQ6hec7O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}